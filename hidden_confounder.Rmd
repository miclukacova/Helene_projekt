---
title: 'Hidden Confounder'
output: html_document
date: "2024-10-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=FALSE}
# Libraries
library(data.table)
library(survival)
#install.packages("rtmle")
#devtools::install_github("tagteam/rtmle")
library(rtmle)
library(ggplot2)
library(microbenchmark)
theme_set(theme_bw())
source("~/Helene_projekt/funcs_for_two_settings.R")
```

In the following we simulate healthcare data from a setting with 4 different types of events: Death, Censoring, confounding Event (L) and Treatment (A). Death and Censoring are terminal events and each individual can experience the confounding Event and Treatment once. The purpose of the following is to investigate how failing to account for the confounder affects the analysis of the data. There is an initial covariate $L0$ that affects the intensities of the different events. *For now it is uniform on $(0,1)$, but we would like to make it uniform on $(50,60)$ and let it represent age. Furthermore we would like to add an additional binary covariate representing sex.* The intensities of the different events follow the form

$$\lambda_{x}(t) =  R^x(t) \lambda_{0}(x,t)\phi(x,t)$$
The function $R^x(t)$ is an at risk indicator. The baseline hazard is specified as
$$\lambda_{0}(x,t)=\eta_x \nu_x t^{\nu_x-1} $$
And
$$\phi(x,t) = \exp (\beta_{L_0 \to x} L_0 + \beta_{A \to x} A + \beta_{L \to x} L)$$

# Interventions

*1. Use simulation function to generate one dataset of reasonable size $n$.*

```{r}
data0 <- sim_data_setting1(N = 5000, cens = 0)
```

*2. On that data, estimate a marginal intensity/hazard of operation; perhaps use a Weibull regression?*

```{r}
data0[, at_risk_oper := as.numeric(A == 0)]
survfit <- survreg(Surv(Time, Delta == 0) ~ L0 + L, 
                   data = data0[at_risk_oper == 1], 
                   cluster = ID,
                   dist='weibull')
```

I think there might be a problem here, since we have time varying covariates. I am a bit unsure how we can fit a Weibull regression on this. 

*3. Use your simulation function to make a large data set under the intervened intensity*

We first find the estimates from the fitted regression:

```{r}
nu_est <- 1/survfit$scale; nu_est
eta_est <- 1/exp(survfit$coefficients[1]); eta_est
beta_l0_a_est <- - survfit$coefficients[2] / survfit$scale 
beta_l_a_est <- - survfit$coefficients[3] / survfit$scale
```

We generate two large data sets under the intervened intensity. One of them without any operation events. 
```{r}
N <- 5000

data_under_int <- sim_data_setting1(N = N, cens = 0,
                                    eta = c(rep(0.1,3), eta_est),
                                    nu = c(rep(1.1,3), nu_est),
                                    beta_L0_A = beta_l0_a_est,
                                    beta_L_A = beta_l_a_est)


data_under_int_no_op <- sim_data_setting1(N = N, cens = 0, 
                                          op = 0, eta = c(rep(0.1,3), eta_est),
                                          nu = c(rep(1.1,3), nu_est),
                                          beta_L0_A = beta_l0_a_est,
                                          beta_L_A = beta_l_a_est)
```

*4. Compute the average of subjects dying before some time tau. Do the same in the setting where all operation events are removed.*

We compute the average of subjects dying before some time $\tau$.  
```{r}
tau <- 5
mean(data_under_int[Delta == 1, Time] < tau) # With operations
mean(data_under_int_no_op[Delta == 1, Time] < tau) # No operation
```


```{r}
# We might have problems with time varying covariates - evt prÃ¸v det her
#flexsurv::flexsurvreg(formula = Surv(tstart, tstop, Delta == 0) ~ L0 + L, 
#                      data=set1_data_t,
#                      dist = "weibull") 

#set1_data_t <- trans_int_data(data0)

#set1_data_t[, at_risk_oper := as.numeric(A == 0)]
#set1_data_t[, at_risk_cov := as.numeric(L == 0)]

#survfit_oper <- coxph(Surv(tstart, tstop, Delta == 0) ~ L0 + L, 
#                      data = set1_data_t[at_risk_oper == 1], cluster = ID)
```


